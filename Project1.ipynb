{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f72f561",
   "metadata": {},
   "source": [
    "You can use this notebook to write the ML pipeline for the classification of the galaxies in the GALAXYZOO dataset or create a folder with different files associated to the different steps of the ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be4be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import Random forest classifiers\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02232e77",
   "metadata": {},
   "source": [
    "# Downloading the Galaxy Zoo Dataset\n",
    "\n",
    "You can find the dataset from the Github repository at the url:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8190c",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/galaxy-zoo-the-galaxy-challenge/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aed504-3ba4-4e6c-8cf2-6a7712020c11",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b76134-4824-4115-b3a5-de1e7775b202",
   "metadata": {},
   "source": [
    "##### Create a data frame with columns for objid and the corresponding assest_id.  \n",
    "- asset_id: an integer that corresponds to the filename of the image of a particular galaxy.\n",
    "- objid is the designation of the galaxy, e.g. galaxy 587722981741363294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f46a77d-db1f-42e7-a8ba-f857afbc66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# get the objid and corresponding asset_id from gz2_filename_mapping.csv\n",
    "columns_to_keep = ['objid', 'asset_id']\n",
    "\n",
    "# Read the selected columns from the file\n",
    "name_map = pd.read_csv(\"data/gz2_filename_mapping.csv\", usecols=columns_to_keep)\n",
    "\n",
    "# display the first few rows\n",
    "print(name_map.head(5))\n",
    "\n",
    "name_map.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7deebcc-b02f-4779-bc8e-bfbbedd33e8d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb15ac7-997c-4b6d-bccf-1561fe270f2f",
   "metadata": {},
   "source": [
    "#### Create a data frame with dr7objid and corresponding label. \n",
    "- dr7objid gives the galaxy designation same as objid from the previous data frame.\n",
    "- label correspond to some classification of the galaxy based on its shape and morphology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea2dd3-b5c3-4130-a520-6ab040a9f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns dr7objid and gz2class from zoo2MainSpecz.csv\n",
    "columns_to_keep = ['dr7objid', 'gz2class']\n",
    "\n",
    "# Read the selected columns from the file\n",
    "labels = pd.read_csv(\"data/zoo2MainSpecz.csv\", usecols=columns_to_keep)\n",
    "\n",
    "# change the name of column dr7objid to objid for merging later\n",
    "labels.rename(columns={'dr7objid':'objid'}, inplace=True)\n",
    "\n",
    "# display\n",
    "print(labels.head(5))\n",
    "\n",
    "labels.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad74d7-4b27-47fa-9b20-38c423d4130c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb83bd6-9489-413e-9117-b612c4f8f1b3",
   "metadata": {},
   "source": [
    "1. Convert array of pixels in rows of a tabular dataset,\n",
    "   using single pixels as feature columns and the intensities as values measured "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b5dfd-d587-4fdb-abae-64b3b9f02dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential implementation. Loops through one image at a time. This is embarassingly \n",
    "# parallelizable. The task which here consists of 1. processing images, converting to grayscale \n",
    "# and flattening pixel values is CPU-bound, ie performance is determined promarily by how\n",
    "# CPU can process it in contrast to I/O bound. \n",
    "# We can parallelize using Multiprocessing library or Dask. \n",
    "\n",
    "import os \n",
    "from PIL import Image, ImageOps\n",
    "from numpy import asarray\n",
    "\n",
    "# Directory containing the images\n",
    "image_dir = \"data/images\"\n",
    "\n",
    "# List to store image data \n",
    "image_data = []\n",
    "image_names = []\n",
    "\n",
    "# Iterate over all files in the directory \n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith(('.jpg', '.png')): #filter the image files\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "\n",
    "        # Open image and convert to grayscale\n",
    "        img = Image.open(image_path)\n",
    "        img_gray = ImageOps.grayscale(img)\n",
    "\n",
    "        # Convert to a numpy array and flatter it to 1D\n",
    "        img_array = np.asarray(img_gray).flatten()\n",
    "\n",
    "        #store the image data and filename\n",
    "        image_data.append(img_array)\n",
    "\n",
    "        # Extract the base name without the extension\n",
    "        image_name = os.path.splitext(filename)[0] # Get only the root, ie w/o extension\n",
    "    \n",
    "        image_names.append(image_name)\n",
    "\n",
    "# convert to DataFrame\n",
    "image_data = pd.DataFrame(image_data)\n",
    "image_data.insert(0, \"asset_id\", image_names) # NOTE: asset_id values are object type. Need to convert to int64 before merging later. \n",
    "# print(image_data['asset_id'].dtype)\n",
    "\n",
    "#display the data frame\n",
    "print(image_data.head())\n",
    "image_data.info()\n",
    "\n",
    "# Save to CSV\n",
    "#image_data(\"image_pixel_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7633cf-eddf-465e-bb60-e857ebbafb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge labels and name_map dataframes to map asset_id to gz2class\n",
    "# merge based on objid. use an inner join (only matching rows) \n",
    "# since only a subset of points in labels are in name_map, ann inner join \n",
    "# will include the rows from name_map that have matching gz2class values\n",
    "# this will avoid NaNs\n",
    "\n",
    "labels_mapped = pd.merge(name_map, labels, on='objid', how='inner' ) \n",
    "\n",
    "print(labels_mapped.head(5))\n",
    "\n",
    "labels_mapped.info() # should have the same number of rows as the dataframe labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019b719-248f-40c9-aa87-8b08656691eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge labels_mapped with image_data to insert gz2class columnt to the latter \n",
    "# Merge based on asset_id and use an inner join. image_data which is our \n",
    "# main data frame will only have, in general, a subset of data points (galaxies)\n",
    "# in labels_mapped. \n",
    "\n",
    "# convert asset_id values in image_data from object to int64 before mergeing\n",
    "image_data['asset_id'] = labels_mapped['asset_id'].astype(int)\n",
    "\n",
    "#merge\n",
    "galaxy_data = pd.merge(labels_mapped, image_data, on='asset_id', how='inner' ) \n",
    "\n",
    "# Move gz2class to the last position to serve as labels\n",
    "galaxy_data['gz2class'] = galaxy_data.pop('gz2class')  \n",
    "\n",
    "# print\n",
    "print(galaxy_data.head(5))\n",
    "\n",
    "galaxy_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c395322-29a3-4aa4-811e-07c957af09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed, compute\n",
    "import os \n",
    "from PIL import Image, ImageOps\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14594255-80eb-45e7-8a75-78e71866d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel implementation of processing the images with DASK\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = \"data/images\"\n",
    "\n",
    "# Get list of image file paths\n",
    "image_files = [\n",
    "    os.path.join(image_dir, f) for f in os.listdir(image_dir)\n",
    "    if f.endswith(('.png', '.jpg'))\n",
    "]\n",
    "\n",
    "# Function to process a single image (Dask version)\n",
    "@delayed\n",
    "def process_image(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        img_gray = ImageOps.grayscale(img)\n",
    "        img_array = np.asarray(img_gray).flatten()\n",
    "        filename = os.path.basename(image_path)\n",
    "        return os.path.splitext(filename)[0], img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Parallel execution using Dask\n",
    "delayed_results = [process_image(img) for img in image_files]\n",
    "results = compute(*delayed_results)\n",
    "\n",
    "# Filter out failed reads\n",
    "results = [res for res in results if res is not None]\n",
    "\n",
    "# Convert to Dask DataFrame\n",
    "image_names, data = zip(*results)\n",
    "image_data = pd.DataFrame(data)\n",
    "image_data.insert(0, \"asset_id\", image_names)\n",
    "\n",
    "print(galaxy_data.head())\n",
    "\n",
    "image_data.info()\n",
    "# Save to CSV\n",
    "#df.to_csv(\"image_pixel_data.csv\", index=False)\n",
    "#print(\"Processing complete. Data saved to 'image_pixel_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc66c7-4ae4-4a94-a49b-073d716736ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge labels_mapped with image_data to insert gz2class columnt to the latter \n",
    "# Merge based on asset_id and use an inner join. image_data which is our \n",
    "# main data frame will only have, in general, a subset of data points (galaxies)\n",
    "# in labels_mapped. \n",
    "\n",
    "# convert asset_id values in image_data from object to int64 before mergeing\n",
    "image_data['asset_id'] = labels_mapped['asset_id'].astype(int)\n",
    "\n",
    "#merge\n",
    "galaxy_data = pd.merge(labels_mapped, image_data, on='asset_id', how='inner') \n",
    "\n",
    "# Move gz2class to the last position to serve as labels\n",
    "galaxy_data['gz2class'] = galaxy_data.pop('gz2class')  \n",
    "\n",
    "# print\n",
    "print(galaxy_data.head(5))\n",
    "\n",
    "galaxy_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5ecb25-a90d-453d-b5be-ff1d3f5b3251",
   "metadata": {},
   "source": [
    "### 2. Perform EDA and feature preprocessing\n",
    "#### 2.1 Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70aecf-4e70-41bf-8ec3-999b24cc9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print\n",
    "#print(galaxy_data.head(4))\n",
    "\n",
    "print(galaxy_data.shape)  # Check dimensions\n",
    "\n",
    "galaxy_data.info()# Check data types & missing values\n",
    "\n",
    "print(galaxy_data.describe())  # Get summary stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0105d921",
   "metadata": {},
   "source": [
    "# Utility function to read Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using PILLOW to convert images to array\n",
    "from PIL import Image, ImageOps\n",
    "from numpy import asarray\n",
    " \n",
    " \n",
    "# load the image and convert into numpy array\n",
    "img = Image.open('data/images/6994.jpg') \n",
    "\n",
    "img_gray = ImageOps.grayscale(img)\n",
    "\n",
    "#img_gray.show() #to check it become gray\n",
    " \n",
    "# asarray() class is used to convert\n",
    "# PIL images into NumPy arraystotal_classifications\n",
    "numpydata = asarray(img_gray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d16d55",
   "metadata": {},
   "source": [
    "# EDA, feature preprocessing and classification\n",
    "\n",
    "1. Convert array of pixels in rows of a tabular dataset,\n",
    "   using single pixels as feature columns and the intensities as values measured \n",
    "   \n",
    "   \n",
    "2. Perform EDA and feature preprocessing\n",
    "\n",
    "3. Estimate the symmetry of the preprocessed images with respect to 12 axes and add this info to the original data\n",
    "\n",
    "3. Test how much you can reduce the dimensions of the problem with one algorithm between (PCA, kPCA ..)\n",
    "4. Check how many clusters can be associated to the data points joint distribution using tSNE or UMAP\n",
    "\n",
    "5. Build the classifier using Random Forest (play with different  depth and number of trees) or SVC\n",
    "\n",
    "6. Train the classifier\n",
    "\n",
    "7. Predict the class labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346703d",
   "metadata": {},
   "source": [
    "# Evaluate the accuracy of the Classifier\n",
    "## Plot Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511344d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:group_project_env]",
   "language": "python",
   "name": "conda-env-group_project_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
